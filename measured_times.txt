===> Using 'cuda' for computation.

####################################################
#               Procesing ResNetModel                 #
####################################################
Converting ResNetModel to TensorRT FP32...
Converting ResNetModel to TensorRT FP16...
Converting ResNetModel to TensorRT INT8...

Testing inference times over PyTorch FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (713.6965589735244) and std (214.11265622551164)  in ms
        -Model Throughput (FPS)->1.4094799522292842
        -Quality PSNR (24.876483917236328)

Testing inference times over PyTorch FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (316.87693888346354) and std (95.06439351246223)  in ms
        -Model Throughput (FPS)->3.165328409886266
        -Quality PSNR (24.880208333333332)

Testing inference times over TensorRT FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (654.9191507975261) and std (196.47746079143502)  in ms
        -Model Throughput (FPS)->1.5293166510130296
        -Quality PSNR (24.876483917236328)

Testing inference times over TensorRT FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (199.56831648084852) and std (59.87106907273105)  in ms
        -Model Throughput (FPS)->5.032866652979478
        -Quality PSNR (24.87505594889323)

Testing inference times over TensorRT INT8 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (131.79612053765192) and std (39.53926034010163)  in ms
        -Model Throughput (FPS)->7.635942883937202
        -Quality PSNR (11.173668225606283)

####################################################
#               Procesing ResNetModel_LateFusion                 #
####################################################
Converting ResNetModel_LateFusion to TensorRT FP32...
Converting ResNetModel_LateFusion to TensorRT FP16...
Converting ResNetModel_LateFusion to TensorRT INT8...

Testing inference times over PyTorch FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (709.6418721516927) and std (212.89891094626057)  in ms
        -Model Throughput (FPS)->1.4161138961395785
        -Quality PSNR (29.06027348836263)

Testing inference times over PyTorch FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (309.73040534125437) and std (92.92092223219956)  in ms
        -Model Throughput (FPS)->3.237385289359356
        -Quality PSNR (29.0625)

Testing inference times over TensorRT FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (654.3450202094184) and std (196.30445041629125)  in ms
        -Model Throughput (FPS)->1.532311480109916
        -Quality PSNR (29.06027348836263)

Testing inference times over TensorRT FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (198.53113928900825) and std (59.55962444158243)  in ms
        -Model Throughput (FPS)->5.057825665454412
        -Quality PSNR (29.064523696899414)

Testing inference times over TensorRT INT8 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (132.06692911783855) and std (39.6204786973482)  in ms
        -Model Throughput (FPS)->7.618972329258744
        -Quality PSNR (11.676202456156412)

####################################################
#               Procesing InceptionModel                 #
####################################################
Converting InceptionModel to TensorRT FP32...
Converting InceptionModel to TensorRT FP16...
Converting InceptionModel to TensorRT INT8...

Testing inference times over PyTorch FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (206.09207712809246) and std (61.8279550041952)  in ms
        -Model Throughput (FPS)->4.852217646331363
        -Quality PSNR (26.970356623331707)

Testing inference times over PyTorch FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (127.51789923773872) and std (38.25595796476287)  in ms
        -Model Throughput (FPS)->7.842415447443938
        -Quality PSNR (26.973958333333332)

Testing inference times over TensorRT FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (170.04439798990884) and std (51.01354605450997)  in ms
        -Model Throughput (FPS)->5.913935103256817
        -Quality PSNR (26.970356623331707)

Testing inference times over TensorRT FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (67.36133202446831) and std (20.208965646084728)  in ms
        -Model Throughput (FPS)->15.038388494953578
        -Quality PSNR (26.98090934753418)

Testing inference times over TensorRT INT8 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (47.43168178134494) and std (14.230125132923376)  in ms
        -Model Throughput (FPS)->21.423570185046593
        -Quality PSNR (11.28984022140503)

####################################################
#               Procesing InceptionModel_LateFusion                 #
####################################################
Converting InceptionModel_LateFusion to TensorRT FP32...
Converting InceptionModel_LateFusion to TensorRT FP16...
Converting InceptionModel_LateFusion to TensorRT INT8...

Testing inference times over PyTorch FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (205.77322540283203) and std (61.732344449212604)  in ms
        -Model Throughput (FPS)->4.86435611575846
        -Quality PSNR (28.551069895426433)

Testing inference times over PyTorch FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (126.81714664035373) and std (38.04553973950078)  in ms
        -Model Throughput (FPS)->7.888571638574177
        -Quality PSNR (28.557291666666668)

Testing inference times over TensorRT FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (174.0003409491645) and std (52.20025377334641)  in ms
        -Model Throughput (FPS)->5.774612847430889
        -Quality PSNR (28.551069895426433)

Testing inference times over TensorRT FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (67.97459250556098) and std (20.392743201650227)  in ms
        -Model Throughput (FPS)->14.883968111865956
        -Quality PSNR (28.55426279703776)

Testing inference times over TensorRT INT8 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (47.775210783216686) and std (14.333873628704184)  in ms
        -Model Throughput (FPS)->21.26019302693908
        -Quality PSNR (11.3268035252889)

####################################################
#               Procesing CNNModel                 #
####################################################
Converting CNNModel to TensorRT FP32...
Converting CNNModel to TensorRT FP16...
Converting CNNModel to TensorRT INT8...

Testing inference times over PyTorch FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (676.9898613823784) and std (203.09934810863805)  in ms
        -Model Throughput (FPS)->1.4818924420080957
        -Quality PSNR (28.138348897298176)

Testing inference times over PyTorch FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (297.5791741265191) and std (89.27524089772663)  in ms
        -Model Throughput (FPS)->3.3695695678036075
        -Quality PSNR (28.140625)

Testing inference times over TensorRT FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (624.1758117675781) and std (187.25496369845808)  in ms
        -Model Throughput (FPS)->1.605219756491499
        -Quality PSNR (28.1383482615153)

Testing inference times over TensorRT FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (191.39212714301215) and std (57.41810999376537)  in ms
        -Model Throughput (FPS)->5.246914877540829
        -Quality PSNR (28.132497151692707)

Testing inference times over TensorRT INT8 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (0.8212302241060468) and std (0.25563751328355055)  in ms
        -Model Throughput (FPS)->2301.679215049279
        -Quality PSNR (11.548112233479818)

####################################################
#               Procesing CNNModel_LateFusion                 #
####################################################
Converting CNNModel_LateFusion to TensorRT FP32...
Converting CNNModel_LateFusion to TensorRT FP16...
Converting CNNModel_LateFusion to TensorRT INT8...

Testing inference times over PyTorch FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (674.4915452745225) and std (202.3500568856989)  in ms
        -Model Throughput (FPS)->1.490537630650067
        -Quality PSNR (28.88698450724284)

Testing inference times over PyTorch FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (290.5791920979818) and std (87.17528791609847)  in ms
        -Model Throughput (FPS)->3.4503814673880977
        -Quality PSNR (28.875)

Testing inference times over TensorRT FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (623.3835984971788) and std (187.0174458618178)  in ms
        -Model Throughput (FPS)->1.6072911805672179
        -Quality PSNR (28.88698387145996)

Testing inference times over TensorRT FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (190.20902065700955) and std (57.06319201706028)  in ms
        -Model Throughput (FPS)->5.27953754502481
        -Quality PSNR (28.87971369425456)

Testing inference times over TensorRT INT8 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (127.67194951375326) and std (38.302549378732735)  in ms
        -Model Throughput (FPS)->7.8834847514307285
        -Quality PSNR (11.66981569925944)

####################################################
#               Procesing AttentionModel                 #
####################################################
Converting AttentionModel to TensorRT FP32...
[TensorRT] ERROR: 3: [network.cpp::addPoolingNd::884] Error Code 3: Internal Error (Parameter check failed at: optimizer/api/network.cpp::addPoolingNd::884, condition: allDimsGtEq(windowSize, 1) && volume(windowSize) < MAX_KERNEL_DIMS_PRODUCT(nbSpatialDims)
)

 ######################## EXCEPTION OCURRED ##################################
'NoneType' object has no attribute 'stride_nd'
################################### END EXCEPTION ###############################


Testing inference times over PyTorch FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (557.9050306532118) and std (167.37596809272432)  in ms
        -Model Throughput (FPS)->1.800494063719683
        -Quality PSNR (28.10034116109212)

Testing inference times over PyTorch FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (276.3602077907986) and std (82.90840814791437)  in ms
        -Model Throughput (FPS)->3.628099272064351
        -Quality PSNR (28.104166666666668)

####################################################
#               Procesing AttentionModel_LateFusion                 #
####################################################
Converting AttentionModel_LateFusion to TensorRT FP32...
[TensorRT] ERROR: 3: [network.cpp::addPoolingNd::884] Error Code 3: Internal Error (Parameter check failed at: optimizer/api/network.cpp::addPoolingNd::884, condition: allDimsGtEq(windowSize, 1) && volume(windowSize) < MAX_KERNEL_DIMS_PRODUCT(nbSpatialDims)
)

 ######################## EXCEPTION OCURRED ##################################
'NoneType' object has no attribute 'stride_nd'
################################### END EXCEPTION ###############################


Testing inference times over PyTorch FP32 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP32] Inference time mean (559.3840549045138) and std (167.8175970904957)  in ms
        -Model Throughput (FPS)->1.7971972148495867
        -Quality PSNR (29.166518529256184)

Testing inference times over PyTorch FP16 ...
        -Warming up GPU...
        -Measuring times over 100 iterations...
        -[MODEL IN FP16] Inference time mean (275.3964569091797) and std (82.61926734019029)  in ms
        -Model Throughput (FPS)->3.642987085009465
        -Quality PSNR (29.171875)
